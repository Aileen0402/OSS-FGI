{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import pandas as pd\n",
    "from ast import literal_eval\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy as np\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "# from keras.models import Sequential\n",
    "# from keras.layers import Conv1D, MaxPooling1D, LSTM, Dense, Flatten\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, LSTM, Dense, Flatten\n",
    "import os\n",
    "import openpyxl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data_from_csv(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    return df\n",
    "\n",
    "def xlsx_to_csv_pd(path_xls):\n",
    "    temp = path_xls.rsplit('.', 1)\n",
    "    path_csv = temp[0] + '.csv'\n",
    "    data_xls = pd.read_excel(path_xls, index_col=0)\n",
    "    data_xls.to_csv(path_csv, encoding='utf-8')\n",
    "    return path_csv\n",
    "\n",
    "# Label 0 Is Legiti 1 Is Malware\n",
    "# 4 kinds\n",
    "file_path = 'data/Classifer_BaseOnNPM/embedding/pm_npm/all.csv'  \n",
    "# file_path = 'data/Classifer_BaseOnNPM/embedding/pm_npm/dynamic.csv'  \n",
    "# file_path = 'data/Classifer_BaseOnNPM/embedding/pm_npm/meta4.csv'  \n",
    "# file_path = 'data/Classifer_BaseOnNPM/embedding/pm_npm/static.csv'  \n",
    "\n",
    "# Static without 'no get'\n",
    "# file_path = 'data/Classifer_BaseOnNPM/embedding/pm_static_noget/npm.csv'  \n",
    "\n",
    "df = read_data_from_csv(file_path)\n",
    "\n",
    "# Randomly Select 200 Pieces Each\n",
    "label_0 = df[df['label'] == 0].sample(n=200, random_state=42)\n",
    "label_1 = df[df['label'] == 1].sample(n=200, random_state=42)\n",
    "df = pd.concat([label_0, label_1])\n",
    "# # print(df)\n",
    "\n",
    "df['feature'] = df['feature'].apply(literal_eval)\n",
    "endan=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "def preprocess_data(df):\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    X_expanded = pd.DataFrame(df['feature'].tolist()).astype(float)\n",
    "\n",
    "    X_expanded.fillna(0, inplace=True)  # Replace Nan With 0 You May Need To Choose Other Strategies\n",
    "\n",
    "    X_scaled = scaler.fit_transform(X_expanded)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_scaled, df['label'], test_size=0.2, random_state=42)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "def train_and_predict(model, X_train, X_test, y_train, param_grid):\n",
    "    grid_search = GridSearchCV(model, param_grid, cv=5, scoring='accuracy')\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    best_model = grid_search.best_estimator_\n",
    "    X_test_expanded = X_test.astype(float)\n",
    "    y_pred = best_model.predict(X_test_expanded)\n",
    "\n",
    "    return y_pred, best_model, grid_search.best_params_, grid_search.best_score_\n",
    "\n",
    "# The Main Function Of Machine Learning\n",
    "def traditional_machine():\n",
    "    X = df.iloc[:, :-1].values\n",
    "    y = df.iloc[:, -1].values\n",
    "\n",
    "    X_train_scaled, X_test_scaled, y_train, y_test = preprocess_data(df)\n",
    "\n",
    "    models_and_params = {\n",
    "        LogisticRegression(): {'C': [0.001, 0.01, 0.1, 1, 10, 100]},\n",
    "        DecisionTreeClassifier(): {'max_depth': [None, 10, 20, 30], 'min_samples_split': [2, 5, 10]},\n",
    "        RandomForestClassifier(): {'n_estimators': [50, 100, 200], 'max_depth': [None, 10, 20, 30], 'min_samples_split': [2, 5, 10]},\n",
    "        SVC(): {'C': [0.1, 1, 10], 'kernel': ['linear', 'rbf']},\n",
    "        # # GaussianNB(): {},\n",
    "        KNeighborsClassifier(): {'n_neighbors': [3, 5, 7]},\n",
    "        MLPClassifier(): {'hidden_layer_sizes': [(50,), (100,), (50, 50)]}\n",
    "    }\n",
    "\n",
    "    for model, params in models_and_params.items():\n",
    "        model_name = model.__class__.__name__\n",
    "        print(f\"Tuning {model_name} parameters and performing cross-validation evaluation...\")\n",
    "\n",
    "        y_pred, best_model, best_params, best_score = train_and_predict(model, X_train_scaled, X_test_scaled, y_train, params)\n",
    "\n",
    "        # Output best model and parameters\n",
    "        print(f\"Best model: {best_model}\")\n",
    "        print(f\"Best parameters: {best_params}\")\n",
    "        print(f\"Best cross-validation accuracy: {best_score:.2f}\")\n",
    "\n",
    "        # Evaluate the model\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        precision = precision_score(y_test, y_pred)\n",
    "        recall = recall_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "        print(f\"{model_name} test accuracy: {accuracy:.10f}\")\n",
    "        print(f\"{model_name} Precision: {precision:.10f}\")\n",
    "        print(f\"{model_name} Recall: {recall:.10f}\")\n",
    "        print(f\"{model_name} F1-Score: {f1:.10f}\")\n",
    "        endan.append([model_name,best_params,accuracy,precision,recall,f1])\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        print(f\"{model_name} Confusion Matrix:\\n{cm}\")\n",
    "\n",
    "# Run machine learning\n",
    "traditional_machine()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df.drop('label', axis=1)  \n",
    "X_train = pd.DataFrame(df['feature'].tolist()).astype(float)\n",
    "y_train = df['label']  \n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train_cnn = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
    "X_test_cnn = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
    "\n",
    "X_train_lstm = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
    "X_test_lstm = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
    "\n",
    "y_train_categorical = to_categorical(y_train)\n",
    "y_test_categorical = to_categorical(y_test)\n",
    "\n",
    "param_grid = {\n",
    "    'epochs': [10, 20],\n",
    "    'batch_size': [32, 64],\n",
    "    'filters': [32, 64],\n",
    "    'kernel_size': [3, 5]\n",
    "}\n",
    "\n",
    "\n",
    "best_params = None\n",
    "best_performance = 0.0\n",
    "best_precision = 0\n",
    "best_recall = 0\n",
    "best_f1 = 0\n",
    "\n",
    "for params in ParameterGrid(param_grid):\n",
    "    print(\"Current Parameters:\", params)\n",
    "    \n",
    "    lstm_model = Sequential()\n",
    "    lstm_model.add(LSTM(50, input_shape=(X_train_lstm.shape[1], X_train_lstm.shape[2])))\n",
    "    lstm_model.add(Dense(1, activation='sigmoid'))  \n",
    "\n",
    "    lstm_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    lstm_model.fit(X_train_lstm, y_train, epochs=params['epochs'], batch_size=params['batch_size'], validation_split=0.1)\n",
    "\n",
    "    lstm_predictions = lstm_model.predict(X_test_lstm)\n",
    "    lstm_predictions = (lstm_predictions > 0.5).astype(int)  \n",
    "    \n",
    "    lstm_accuracy = accuracy_score(y_test, lstm_predictions)\n",
    "    lstm_precision = precision_score(y_test, lstm_predictions)\n",
    "    lstm_recall = recall_score(y_test, lstm_predictions)\n",
    "    lstm_f1 = f1_score(y_test, lstm_predictions)\n",
    "    \n",
    "    print(\"LSTM Accuracy:\", lstm_accuracy)\n",
    "    print(\"LSTM Precision:\", lstm_precision)\n",
    "    print(\"LSTM Recall:\", lstm_recall)\n",
    "    print(\"LSTM F1-Score:\", lstm_f1)\n",
    "\n",
    "    lstm_cm = confusion_matrix(y_test, lstm_predictions)\n",
    "    print(\"LSTM Confused Matrix:\\n\", lstm_cm)\n",
    "\n",
    "    # Update The Best Performance\n",
    "    if lstm_accuracy > best_performance:\n",
    "        best_performance = lstm_accuracy\n",
    "        best_params = params\n",
    "        best_precision = lstm_precision\n",
    "        best_recall = lstm_recall\n",
    "        best_f1 = lstm_f1\n",
    "\n",
    "print(\"Best Parameters:\", best_params)\n",
    "print(\"Best Performance (Accuracy):\", best_performance)\n",
    "print(\"LSTM Recall:\", best_recall)\n",
    "print(\"LSTM Precision:\", best_precision)\n",
    "print(\"LSTM F1-Score:\", best_f1)\n",
    "endan.append(['LSTM',best_params,best_performance,best_precision,best_recall,best_f1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df.drop('label', axis=1)  \n",
    "X_train = pd.DataFrame(df['feature'].tolist()).astype(float)\n",
    "y_train = df['label']  \n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train_cnn = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
    "X_test_cnn = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
    "\n",
    "y_train_categorical = to_categorical(y_train)\n",
    "y_test_categorical = to_categorical(y_test)\n",
    "\n",
    "\n",
    "param_grid = {\n",
    "    'epochs': [10, 20],\n",
    "    'batch_size': [32, 64],\n",
    "    'filters': [32, 64],\n",
    "    'kernel_size': [3, 5]\n",
    "}\n",
    "best_performance = 0.0\n",
    "best_precision = 0\n",
    "best_recall = 0\n",
    "best_f1 = 0\n",
    "for params in ParameterGrid(param_grid):\n",
    "    print(\"Current Parameters:\", params)\n",
    "    \n",
    "    cnn_model = Sequential()\n",
    "    cnn_model.add(Conv1D(filters=params['filters'], kernel_size=params['kernel_size'], activation='relu', input_shape=(X_train_cnn.shape[1], X_train_cnn.shape[2])))\n",
    "    cnn_model.add(MaxPooling1D(pool_size=2))\n",
    "    cnn_model.add(Flatten())\n",
    "    cnn_model.add(Dense(1, activation='sigmoid'))  \n",
    "\n",
    "    cnn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    cnn_model.fit(X_train_cnn, y_train, epochs=params['epochs'], batch_size=params['batch_size'], validation_split=0.1)\n",
    "    cnn_predictions = cnn_model.predict(X_test_cnn)\n",
    "    cnn_predictions = (cnn_predictions > 0.5).astype(int)  \n",
    "    \n",
    "    cnn_accuracy = accuracy_score(y_test, cnn_predictions)\n",
    "    cnn_precision = precision_score(y_test, cnn_predictions)\n",
    "    cnn_recall = recall_score(y_test, cnn_predictions)\n",
    "    cnn_f1 = f1_score(y_test, cnn_predictions)\n",
    "    \n",
    "    print(\"CNN Accuracy:\", cnn_accuracy)\n",
    "    print(\"CNN Precision:\", cnn_precision)\n",
    "    print(\"CNN Recall:\", cnn_recall)\n",
    "    print(\"CNN F1-Score:\", cnn_f1)\n",
    "\n",
    "    cnn_cm = confusion_matrix(y_test, cnn_predictions)\n",
    "    print(\"CNN Confused Matrix:\\n\", cnn_cm)\n",
    "    \n",
    "    # Update The Best Performance\n",
    "    if cnn_accuracy > best_performance:\n",
    "        best_performance = cnn_accuracy\n",
    "        best_params = params\n",
    "        best_precision = cnn_precision\n",
    "        best_recall = cnn_recall\n",
    "        best_f1 = cnn_f1\n",
    "\n",
    "print(\"Best Parameters:\", best_params)\n",
    "print(\"Best Performance (Accuracy):\", best_performance)\n",
    "print(\"CNN Recall:\", best_recall)\n",
    "print(\"CNN Precision:\", best_precision)\n",
    "print(\"CNN F1-Score:\", best_f1)\n",
    "endan.append(['CNN',best_params,best_performance,best_precision,best_recall,best_f1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(endan)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
