{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import openpyxl\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import openpyxl\n",
    "def xlsx_to_csv_pd(path_xls):\n",
    "    temp = path_xls.rsplit('.', 1)\n",
    "    path_csv = temp[0] + '.csv'\n",
    "    data_xls = pd.read_excel(path_xls, index_col=0)\n",
    "    data_xls.to_csv(path_csv, encoding='utf-8')\n",
    "    return path_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all\n",
    "path_xls = 'data/Classifer_BaseOnNPM/word/npm_all.csv'\n",
    "# static\n",
    "# path_xls = 'data/Classifer_BaseOnNPM/word/npm_static_except_noget.csv'\n",
    "\n",
    "# path_csv = xlsx_to_csv_pd(path_xls)\n",
    "df = pd.read_csv(path_xls, dtype=str)\n",
    "\n",
    "# Change different columns here to select vectors\n",
    "# Using 'description' as an example here\n",
    "data = df['des']\n",
    "# Split the dataset into training and testing sets\n",
    "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n",
    "# Tokenization, convert to string to remove NaN\n",
    "tokenized_corpus = [word_tokenize(str(sentence).lower()) for sentence in tqdm(train_data) if not pd.isnull(sentence)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_sizes = [50, 100, 200]\n",
    "window_sizes = [5, 10, 15]\n",
    "min_counts = [1, 5, 10]\n",
    "\n",
    "best_model = None\n",
    "best_params = {'vector_size': None, 'window': None, 'min_count': None}\n",
    "best_similarity = 0.0\n",
    "\n",
    "# Create A Word2 Vec Model And Directly Build A Vocabulary\n",
    "model = Word2Vec(sentences=tokenized_corpus, vector_size=50, window=5, min_count=1, workers=4)\n",
    "model.build_vocab(tokenized_corpus)\n",
    "\n",
    "for vector_size in vector_sizes:\n",
    "    for window_size in window_sizes:\n",
    "        for min_count_value in min_counts:\n",
    "            # Update\n",
    "\n",
    "            model.vector_size = vector_size\n",
    "            # It Defines The Size Of The Context Window That The Model Considers During The Training Process The Larger The Window The More Context And The More Comprehensive The Semantics\n",
    "            model.window = window_size\n",
    "            # Define The Minimum Word Frequency Words Below Word Frequency Are Ignored\n",
    "            model.min_count = min_count_value\n",
    "\n",
    "            model.train(tokenized_corpus, total_examples=len(tokenized_corpus), epochs=10)\n",
    "\n",
    "            word_vectors = model.wv\n",
    "\n",
    "            similarity_matrix = cosine_similarity([word_vectors[word] for word in word_vectors.index_to_key])\n",
    "            avg_similarity = similarity_matrix.mean()\n",
    "\n",
    "            # Save The Best Model And Parameters\n",
    "            if avg_similarity > best_similarity:\n",
    "                best_similarity = avg_similarity\n",
    "                best_model = model\n",
    "                best_params['vector_size'] = vector_size\n",
    "                best_params['window'] = window_size\n",
    "                best_params['min_count'] = min_count_value\n",
    "                \n",
    "print(\"Best Parameters:\")\n",
    "print(best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using The Best Model For Word Embedding\n",
    "word_embeddings = {word: best_model.wv[word] for word in best_model.wv.index_to_key}\n",
    "\n",
    "def vectorize_text(text, embeddings):\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    vectorized_text = [embeddings[word] for word in tokens if word in embeddings]\n",
    "    return vectorized_text\n",
    "\n",
    "all_data_vectorized_des = {}\n",
    "con_des = []\n",
    "for sentence in tqdm(data):\n",
    "    # If The Presence Is Not A Null Value Na N Perform Subsequent Operations\n",
    "    if not pd.isnull(sentence):\n",
    "        sentence_vector = vectorize_text(str(sentence), word_embeddings)\n",
    "\n",
    "        if sentence_vector:\n",
    "            average_vector = np.mean(sentence_vector, axis=0)\n",
    "            all_data_vectorized_des[sentence] = average_vector\n",
    "            con_des.append(average_vector)\n",
    "        else:\n",
    "            # If No Word Vectors Are Found In The Sentence Consider Using All Zero Vectors Or Other Methods To Handle It\n",
    "            all_data_vectorized_des[sentence] = np.zeros(50)\n",
    "            con_des.append(np.zeros(50))\n",
    "\n",
    "print(len(con_des))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace Different Columns Here To Select Vectors\n",
    "data = df['aut_main']\n",
    "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n",
    "tokenized_corpus = [word_tokenize(str(sentence).lower()) for sentence in tqdm(train_data) if not pd.isnull(sentence)]\n",
    "\n",
    "vector_sizes = [50, 100, 200]\n",
    "window_sizes = [5, 10, 15]\n",
    "min_counts = [1, 5, 10]\n",
    "\n",
    "best_model = None\n",
    "best_params = {'vector_size': None, 'window': None, 'min_count': None}\n",
    "best_similarity = 0.0\n",
    "\n",
    "model = Word2Vec(sentences=tokenized_corpus, vector_size=50, window=5, min_count=1, workers=4)\n",
    "model.build_vocab(tokenized_corpus)\n",
    "\n",
    "for vector_size in vector_sizes:\n",
    "    for window_size in window_sizes:\n",
    "        for min_count_value in min_counts:\n",
    "\n",
    "            model.vector_size = vector_size\n",
    "            model.window = window_size\n",
    "            model.min_count = min_count_value\n",
    "\n",
    "            model.train(tokenized_corpus, total_examples=len(tokenized_corpus), epochs=10)\n",
    "\n",
    "            word_vectors = model.wv\n",
    "            similarity_matrix = cosine_similarity([word_vectors[word] for word in word_vectors.index_to_key])\n",
    "            avg_similarity = similarity_matrix.mean()\n",
    "\n",
    "            if avg_similarity > best_similarity:\n",
    "                best_similarity = avg_similarity\n",
    "                best_model = model\n",
    "                best_params['vector_size'] = vector_size\n",
    "                best_params['window'] = window_size\n",
    "                best_params['min_count'] = min_count_value\n",
    "\n",
    "print(\"Best Parameters:\")\n",
    "print(best_params)\n",
    "\n",
    "word_embeddings = {word: best_model.wv[word] for word in best_model.wv.index_to_key}\n",
    "\n",
    "\n",
    "def vectorize_text(text, embeddings):\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    vectorized_text = [embeddings[word] for word in tokens if word in embeddings]\n",
    "    return vectorized_text\n",
    "\n",
    "\n",
    "all_data_vectorized_aumain = {}\n",
    "con_aumain = []\n",
    "for sentence in tqdm(data):\n",
    "    \n",
    "    if not pd.isnull(sentence):\n",
    "        sentence_vector = vectorize_text(str(sentence), word_embeddings)\n",
    "        if sentence_vector:\n",
    "            average_vector = np.mean(sentence_vector, axis=0)\n",
    "            all_data_vectorized_aumain[sentence] = average_vector\n",
    "            con_aumain.append(average_vector)\n",
    "        else:\n",
    "            \n",
    "            all_data_vectorized_aumain[sentence] = np.zeros(50)\n",
    "            con_aumain.append(np.zeros(50))\n",
    "\n",
    "print(len(con_aumain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df['url_git']\n",
    "\n",
    "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "tokenized_corpus = [word_tokenize(str(sentence).lower()) for sentence in tqdm(train_data) if not pd.isnull(sentence)]\n",
    "\n",
    "\n",
    "vector_sizes = [50, 100, 200]\n",
    "window_sizes = [5, 10, 15]\n",
    "min_counts = [1, 5, 10]\n",
    "best_model = None\n",
    "best_params = {'vector_size': None, 'window': None, 'min_count': None}\n",
    "best_similarity = 0.0\n",
    "\n",
    "\n",
    "model = Word2Vec(sentences=tokenized_corpus, vector_size=50, window=5, min_count=1, workers=4)\n",
    "model.build_vocab(tokenized_corpus)\n",
    "\n",
    "for vector_size in vector_sizes:\n",
    "    for window_size in window_sizes:\n",
    "        for min_count_value in min_counts:\n",
    "\n",
    "            model.vector_size = vector_size\n",
    "            model.window = window_size\n",
    "            model.min_count = min_count_value\n",
    "            model.train(tokenized_corpus, total_examples=len(tokenized_corpus), epochs=10)\n",
    "\n",
    "            \n",
    "            word_vectors = model.wv\n",
    "            similarity_matrix = cosine_similarity([word_vectors[word] for word in word_vectors.index_to_key])\n",
    "            avg_similarity = similarity_matrix.mean()\n",
    "\n",
    "            \n",
    "            if avg_similarity > best_similarity:\n",
    "                best_similarity = avg_similarity\n",
    "                best_model = model\n",
    "                best_params['vector_size'] = vector_size\n",
    "                best_params['window'] = window_size\n",
    "                best_params['min_count'] = min_count_value\n",
    "\n",
    "\n",
    "print(\"Best Parameters:\")\n",
    "print(best_params)\n",
    "\n",
    "word_embeddings = {word: best_model.wv[word] for word in best_model.wv.index_to_key}\n",
    "def vectorize_text(text, embeddings):\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    vectorized_text = [embeddings[word] for word in tokens if word in embeddings]\n",
    "    return vectorized_text\n",
    "\n",
    "\n",
    "all_data_vectorized_url = {}\n",
    "con_url = []\n",
    "for sentence in tqdm(data):\n",
    "    \n",
    "    if not pd.isnull(sentence):\n",
    "        if sentence_vector:\n",
    "            average_vector = np.mean(sentence_vector, axis=0)\n",
    "            all_data_vectorized_url[sentence] = average_vector\n",
    "            con_url.append(average_vector)\n",
    "        else:\n",
    "            \n",
    "            all_data_vectorized_url[sentence] = np.zeros(50)\n",
    "            con_url.append(np.zeros(50))\n",
    "print(len(con_url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = df['dep_num']\n",
    "\n",
    "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "tokenized_corpus = [word_tokenize(str(sentence).lower()) for sentence in tqdm(train_data) if not pd.isnull(sentence)]\n",
    "\n",
    "\n",
    "vector_sizes = [50, 100, 200]\n",
    "window_sizes = [5, 10, 15]\n",
    "min_counts = [1, 5, 10]\n",
    "\n",
    "best_model = None\n",
    "best_params = {'vector_size': None, 'window': None, 'min_count': None}\n",
    "best_similarity = 0.0\n",
    "\n",
    "\n",
    "model = Word2Vec(sentences=tokenized_corpus, vector_size=50, window=5, min_count=1, workers=4)\n",
    "model.build_vocab(tokenized_corpus)\n",
    "\n",
    "\n",
    "for vector_size in vector_sizes:\n",
    "    for window_size in window_sizes:\n",
    "        for min_count_value in min_counts:\n",
    "            \n",
    "\n",
    "            model.vector_size = vector_size\n",
    "            \n",
    "            model.window = window_size\n",
    "            \n",
    "            model.min_count = min_count_value\n",
    "\n",
    "            \n",
    "            model.train(tokenized_corpus, total_examples=len(tokenized_corpus), epochs=10)\n",
    "\n",
    "            \n",
    "            word_vectors = model.wv\n",
    "\n",
    "            \n",
    "            similarity_matrix = cosine_similarity([word_vectors[word] for word in word_vectors.index_to_key])\n",
    "            avg_similarity = similarity_matrix.mean()\n",
    "\n",
    "            \n",
    "            if avg_similarity > best_similarity:\n",
    "                best_similarity = avg_similarity\n",
    "                best_model = model\n",
    "                best_params['vector_size'] = vector_size\n",
    "                best_params['window'] = window_size\n",
    "                best_params['min_count'] = min_count_value\n",
    "\n",
    "\n",
    "print(\"Best Parameters:\")\n",
    "print(best_params)\n",
    "\n",
    "\n",
    "word_embeddings = {word: best_model.wv[word] for word in best_model.wv.index_to_key}\n",
    "\n",
    "\n",
    "def vectorize_text(text, embeddings):\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    vectorized_text = [embeddings[word] for word in tokens if word in embeddings]\n",
    "    return vectorized_text\n",
    "\n",
    "\n",
    "all_data_vectorized_dep = {}\n",
    "con_dep = []\n",
    "for sentence in tqdm(data):\n",
    "    \n",
    "    if not pd.isnull(sentence):\n",
    "        sentence_vector = vectorize_text(str(sentence), word_embeddings)\n",
    "        if sentence_vector:\n",
    "            average_vector = np.mean(sentence_vector, axis=0)\n",
    "            all_data_vectorized_dep[sentence] = average_vector\n",
    "            con_dep.append(average_vector)\n",
    "        else:\n",
    "            \n",
    "            all_data_vectorized_dep[sentence] = np.zeros(50)\n",
    "            con_dep.append(np.zeros(50))\n",
    "print(len(con_dep))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = df['static_APIs']\n",
    "\n",
    "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "tokenized_corpus = [word_tokenize(str(sentence).lower()) for sentence in tqdm(train_data) if not pd.isnull(sentence)]\n",
    "\n",
    "\n",
    "vector_sizes = [50, 100, 200]\n",
    "window_sizes = [5, 10, 15]\n",
    "min_counts = [1, 5, 10]\n",
    "\n",
    "best_model = None\n",
    "best_params = {'vector_size': None, 'window': None, 'min_count': None}\n",
    "best_similarity = 0.0\n",
    "\n",
    "\n",
    "model = Word2Vec(sentences=tokenized_corpus, vector_size=50, window=5, min_count=1, workers=4)\n",
    "model.build_vocab(tokenized_corpus)\n",
    "\n",
    "\n",
    "for vector_size in vector_sizes:\n",
    "    for window_size in window_sizes:\n",
    "        for min_count_value in min_counts:\n",
    "            \n",
    "\n",
    "            model.vector_size = vector_size\n",
    "            \n",
    "            model.window = window_size\n",
    "            \n",
    "            model.min_count = min_count_value\n",
    "\n",
    "            \n",
    "            model.train(tokenized_corpus, total_examples=len(tokenized_corpus), epochs=10)\n",
    "\n",
    "            \n",
    "            word_vectors = model.wv\n",
    "\n",
    "            \n",
    "            similarity_matrix = cosine_similarity([word_vectors[word] for word in word_vectors.index_to_key])\n",
    "            avg_similarity = similarity_matrix.mean()\n",
    "\n",
    "            \n",
    "            if avg_similarity > best_similarity:\n",
    "                best_similarity = avg_similarity\n",
    "                best_model = model\n",
    "                best_params['vector_size'] = vector_size\n",
    "                best_params['window'] = window_size\n",
    "                best_params['min_count'] = min_count_value\n",
    "\n",
    "\n",
    "print(\"Best Parameters:\")\n",
    "print(best_params)\n",
    "\n",
    "\n",
    "word_embeddings = {word: best_model.wv[word] for word in best_model.wv.index_to_key}\n",
    "\n",
    "\n",
    "def vectorize_text(text, embeddings):\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    vectorized_text = [embeddings[word] for word in tokens if word in embeddings]\n",
    "    return vectorized_text\n",
    "\n",
    "\n",
    "all_data_vectorized_staticAPI = {}\n",
    "con_static = []\n",
    "for sentence in tqdm(data):\n",
    "    \n",
    "    if not pd.isnull(sentence):\n",
    "        sentence_vector = vectorize_text(str(sentence), word_embeddings)\n",
    "        if sentence_vector:\n",
    "            average_vector = np.mean(sentence_vector, axis=0)\n",
    "            all_data_vectorized_staticAPI[sentence] = average_vector\n",
    "            con_static.append(average_vector)\n",
    "        else:\n",
    "            \n",
    "            all_data_vectorized_staticAPI[sentence] = np.zeros(50)\n",
    "            con_static.append(np.zeros(50))\n",
    "print(len(con_static))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = df['Dynamic_APIs']\n",
    "\n",
    "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "tokenized_corpus = [word_tokenize(str(sentence).lower()) for sentence in tqdm(train_data) if not pd.isnull(sentence)]\n",
    "\n",
    "\n",
    "vector_sizes = [50, 100, 200]\n",
    "window_sizes = [5, 10, 15]\n",
    "min_counts = [1, 5, 10]\n",
    "\n",
    "best_model = None\n",
    "best_params = {'vector_size': None, 'window': None, 'min_count': None}\n",
    "best_similarity = 0.0\n",
    "\n",
    "\n",
    "model = Word2Vec(sentences=tokenized_corpus, vector_size=50, window=5, min_count=1, workers=4)\n",
    "model.build_vocab(tokenized_corpus)\n",
    "\n",
    "\n",
    "for vector_size in vector_sizes:\n",
    "    for window_size in window_sizes:\n",
    "        for min_count_value in min_counts:\n",
    "            \n",
    "\n",
    "            model.vector_size = vector_size\n",
    "            \n",
    "            model.window = window_size\n",
    "            \n",
    "            model.min_count = min_count_value\n",
    "\n",
    "            \n",
    "            model.train(tokenized_corpus, total_examples=len(tokenized_corpus), epochs=10)\n",
    "\n",
    "            \n",
    "            word_vectors = model.wv\n",
    "\n",
    "            \n",
    "            similarity_matrix = cosine_similarity([word_vectors[word] for word in word_vectors.index_to_key])\n",
    "            avg_similarity = similarity_matrix.mean()\n",
    "\n",
    "            \n",
    "            if avg_similarity > best_similarity:\n",
    "                best_similarity = avg_similarity\n",
    "                best_model = model\n",
    "                best_params['vector_size'] = vector_size\n",
    "                best_params['window'] = window_size\n",
    "                best_params['min_count'] = min_count_value\n",
    "\n",
    "\n",
    "print(\"Best Parameters:\")\n",
    "print(best_params)\n",
    "\n",
    "\n",
    "word_embeddings = {word: best_model.wv[word] for word in best_model.wv.index_to_key}\n",
    "\n",
    "\n",
    "def vectorize_text(text, embeddings):\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    vectorized_text = [embeddings[word] for word in tokens if word in embeddings]\n",
    "    return vectorized_text\n",
    "\n",
    "\n",
    "all_data_vectorized_DynaAPI = {}\n",
    "con_Dynamic = []\n",
    "for sentence in tqdm(data):\n",
    "    \n",
    "    if not pd.isnull(sentence):\n",
    "        sentence_vector = vectorize_text(str(sentence), word_embeddings)\n",
    "        if sentence_vector:\n",
    "            average_vector = np.mean(sentence_vector, axis=0)\n",
    "            all_data_vectorized_DynaAPI[sentence] = average_vector\n",
    "            con_Dynamic.append(average_vector)\n",
    "        else:\n",
    "            \n",
    "            all_data_vectorized_DynaAPI[sentence] = np.zeros(50)\n",
    "            con_Dynamic.append(np.zeros(50))\n",
    "        \n",
    "print(len(con_Dynamic))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con_all=[]\n",
    "## all\n",
    "for item1, item2, item3, item4, item5, item6 in zip(con_des, con_aumain, con_url, con_dep, con_static, con_Dynamic):\n",
    "## metadata\n",
    "# for item1, item2, item3, item4 in zip(con_des, con_aumain, con_url, con_dep):\n",
    "## static\n",
    "# for item1 in zip(con_static):\n",
    "## dynamic\n",
    "# for item1 in zip(con_Dynamic):\n",
    "\n",
    "    vector1 = np.array(item1)\n",
    "    vector2 = np.array(item2)\n",
    "    vector3 = np.array(item3)\n",
    "    vector4 = np.array(item4)\n",
    "    vector5 = np.array(item5)\n",
    "    vector6 = np.array(item6)\n",
    "\n",
    "# Horizontal Connection\n",
    "    # all\n",
    "    concatenated_horizontal = np.concatenate((vector1, vector2, vector3, vector4, vector5, vector6))\n",
    "    # metadata\n",
    "    # concatenated_horizontal = np.concatenate((vector1, vector2, vector3, vector4))\n",
    "    # staic/dynamic\n",
    "    # concatenated_horizontal = np.concatenate((vector1))\n",
    "    concatenated_horizontal = concatenated_horizontal.tolist()\n",
    "    con_all.append([concatenated_horizontal])\n",
    "print(len(con_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4 kinds\n",
    "csv_file_path = 'data/Classifer_BaseOnNPM/embedding/pm_npm/all.csv'  \n",
    "# csv_file_path = 'data/Classifer_BaseOnNPM/embedding/pm_npm/dynamic.csv'  \n",
    "# csv_file_path = 'data/Classifer_BaseOnNPM/embedding/pm_npm/meta4.csv'  \n",
    "# csv_file_path = 'data/Classifer_BaseOnNPM/embedding/pm_npm/static.csv'  \n",
    "\n",
    "# Static without 'no get'\n",
    "# csv_file_path = 'data/Classifer_BaseOnNPM/embedding/pm_static_noget/npm.csv'  \n",
    "\n",
    "if os.path.exists(csv_file_path):\n",
    "    os.remove(csv_file_path)\n",
    "# Establish\n",
    "workbook = openpyxl.Workbook()\n",
    "workbook.save(csv_file_path)\n",
    "workbook.close()\n",
    "\n",
    "df_existing = pd.read_csv(csv_file_path)\n",
    "df_new = pd.DataFrame([con_all,df['label']], columns=['feature','label'])\n",
    "df_combined = pd.concat([df_existing, df_new], ignore_index=True)\n",
    "df_combined.to_csv(csv_file_path, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
